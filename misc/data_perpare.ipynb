{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import hashlib\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "\n",
    "data_path = '../../analysis/pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data of alternative sites\n",
    "\n",
    "site_info_alt = pd.read_csv(data_path + '/data-fetching/alternative-sites/site_info_20230101.csv')\n",
    "weather_data_alt = pd.read_csv(data_path + '/data-fetching/alternative-sites/df_weather-20230101-20240501.csv')\n",
    "\n",
    "weather_data_alt.rename(columns={'ts': 'time'}, inplace=True)\n",
    "weather_data_alt['time'] = pd.to_datetime(weather_data_alt['time'])\n",
    "\n",
    "power_data_alt = pd.DataFrame()\n",
    "\n",
    "temp_site_info = []\n",
    "\n",
    "for index, row in site_info_alt.iterrows():\n",
    "    site = row\n",
    "    cluster = 0 # cluster 0 is chosen (= no clustering)\n",
    "    site_id = site['site_id']\n",
    "    \n",
    "    new_entry = {\n",
    "        'site_id': site_id, \n",
    "        'clusters': {\n",
    "            'location': -1\n",
    "        },\n",
    "        'lat': 0, # site['lat'],\n",
    "        'lng': 0, # site['lng'],\n",
    "        'zip': site['zip'],\n",
    "        'country': site['country'],\n",
    "        'kwp': site['kwp'],\n",
    "    }\n",
    "    temp_site_info.append(new_entry)\n",
    "    site_power_data = pd.read_csv(data_path + f'/data-fetching/alternative-sites/df_{site_id}-20230101-20240501.csv')\n",
    "    power_data_alt = pd.concat([power_data_alt, site_power_data])\n",
    "\n",
    "site_info_alt = pd.DataFrame(temp_site_info)\n",
    "power_data_alt['time'] = pd.to_datetime(power_data_alt['time'])\n",
    "\n",
    "print('Alternative sites data loaded')\n",
    "print(f'Power data length: {len(power_data_alt)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data of main sites\n",
    "\n",
    "site_info = pd.read_csv(data_path + '/data-fetching/site_info_20230101.csv')\n",
    "weather_data = pd.read_csv(data_path + '/data-fetching/df_weather-20230101-20240101.csv')\n",
    "clusters = pd.read_csv(data_path + '/site-clustering//closest_sites.csv')\n",
    "\n",
    "weather_data.rename(columns={'ts': 'time'}, inplace=True)\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "\n",
    "power_data = pd.DataFrame()\n",
    "\n",
    "temp_site_info = []\n",
    "\n",
    "for index, row in site_info.iterrows():\n",
    "    site = row\n",
    "    cluster = clusters[clusters['site_id'] == site['site_id']].iloc[0]\n",
    "    site_id = site['site_id']\n",
    "    new_entry = {\n",
    "        'site_id': site_id, \n",
    "        'clusters': {'location': cluster['cluster']},\n",
    "        'lat': 0, # site['lat'],\n",
    "        'lng': 0, # site['lng'],\n",
    "        'zip': site['zip'],\n",
    "        'country': site['country'],\n",
    "        'kwp': site['kwp'],\n",
    "    }\n",
    "    temp_site_info.append(new_entry)\n",
    "    site_power_data = pd.read_csv(data_path + f'/data-fetching/df_{site_id}-20230101-20240101.csv')\n",
    "    power_data = pd.concat([power_data, site_power_data])\n",
    "\n",
    "site_info = pd.DataFrame(temp_site_info)\n",
    "power_data['time'] = pd.to_datetime(power_data['time'])\n",
    "\n",
    "print('Main sites data loaded')\n",
    "print(f'Power data length: {len(power_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat alternative and main sites\n",
    "site_info = pd.concat([site_info, site_info_alt], ignore_index=True)\n",
    "power_data = pd.concat([power_data, power_data_alt], ignore_index=True)\n",
    "weather_data = pd.concat([weather_data, weather_data_alt], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('All data loaded')\n",
    "print(f'Site info length: {len(site_info)}')\n",
    "print(f'Power data length: {len(power_data)}')\n",
    "print(f'Weather data length: {len(weather_data)}')\n",
    "\n",
    "print(site_info)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulated_day_energy(power_values):\n",
    "    # check if any none values\n",
    "    if any([x is None for x in power_values]) or len(power_values) != 96:\n",
    "        return [None] * 96\n",
    "    \n",
    "    power_values = np.array(power_values)\n",
    "\n",
    "    # Calculate the energy for each interval (15 minutes => 0.25 hours)\n",
    "    energy_per_interval = power_values * 0.25\n",
    "\n",
    "    # Calculate the accumulated energy\n",
    "    accumulated_energy = np.cumsum(energy_per_interval)\n",
    "\n",
    "    return accumulated_energy\n",
    "\n",
    "def whole_day_energy(power_values):\n",
    "    # check if any none values\n",
    "    if any([x is None for x in power_values]) or len(power_values) != 96:\n",
    "        return [None] * 96\n",
    "    \n",
    "    power_values = np.array(power_values)\n",
    "\n",
    "    # Calculate the energy for each interval (15 minutes => 0.25 hours)\n",
    "    energy_per_interval = power_values * 0.25\n",
    "\n",
    "    # Calculate the accumulated energy\n",
    "    sm = np.sum(energy_per_interval)\n",
    "    whole_day_energy = [sm] * 96\n",
    "\n",
    "    return whole_day_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'../test_data', exist_ok=True)\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "def process_site(site_id):\n",
    "    print(\"Site ID:\", site_id)\n",
    "    site = site_info[site_info['site_id'] == site_id].iloc[0]\n",
    "    power_data_site = power_data[(power_data['site_id'] == site_id)]\n",
    "    weather_data_site = weather_data[(weather_data['zip'] == site['zip'])]\n",
    "    \n",
    "    power_data_site['time'] = pd.to_datetime(power_data_site['time'])\n",
    "    weather_data_site['time'] = pd.to_datetime(weather_data_site['time'])\n",
    "\n",
    "    power_data_site = power_data_site.sort_values('time')\n",
    "\n",
    "\n",
    "    if len(weather_data_site) != 0:\n",
    "        site['weather_data'] = True\n",
    "        \n",
    "        weather_data_site = weather_data_site.sort_values('time')\n",
    "\n",
    "        duplicated_weather = []\n",
    "\n",
    "        for index, row in weather_data_site.iterrows():\n",
    "            ts = row['time']\n",
    "            duplicated_weather.append(row)\n",
    "            for _ in range(3):\n",
    "                ts += pd.Timedelta(minutes=15)\n",
    "                new_row = row.copy()\n",
    "                new_row['time'] = ts\n",
    "                # Append the duplicated row to the list\n",
    "                duplicated_weather.append(new_row)\n",
    "        \n",
    "        generated_weather = pd.DataFrame(duplicated_weather)\n",
    "        generated_weather['site_id'] = site_id\n",
    "        power_data_site['site_id'] = site_id\n",
    "\n",
    "        df = pd.merge(generated_weather, power_data_site, on=['site_id', 'time'])\n",
    "\n",
    "        df = df[['time', 'site_id', 'value_key', 'avg', 'solar_rad', 'temp', 'app_temp', 'uv', 'precip', 'rh', 'ghi', 'snow_depth', 'clouds']]\n",
    "\n",
    "\n",
    "        weather_columns = ['solar_rad', 'temp', 'app_temp', 'uv', 'precip', 'rh', 'ghi', 'snow_depth', 'clouds']\n",
    "        for row in df.iterrows():\n",
    "            # get row 1h and 2h before\n",
    "            row_1h = df[(df['time'] == row[1]['time'] - pd.Timedelta(hours=1)) & (df['value_key'] == row[1]['value_key'])]\n",
    "            row_2h = df[(df['time'] == row[1]['time'] - pd.Timedelta(hours=2)) & (df['value_key'] == row[1]['value_key'])]\n",
    "            \n",
    "            # set values if data is available\n",
    "            if len(row_1h) != 0:\n",
    "                for column in weather_columns:\n",
    "                    df.loc[row[0], f'{column}_1h'] = row_1h[column].values[0]\n",
    "            else:\n",
    "                for column in weather_columns:\n",
    "                    df.loc[row[0], f'{column}_1h'] = np.nan\n",
    "\n",
    "            if len(row_2h) != 0:\n",
    "                for column in weather_columns:\n",
    "                    df.loc[row[0], f'{column}_2h'] = row_2h[column].values[0]\n",
    "            else:\n",
    "                for column in weather_columns:\n",
    "                    df.loc[row[0], f'{column}_2h'] = np.nan\n",
    "            \n",
    "    else:\n",
    "        df = power_data_site[['time', 'site_id', 'value_key', 'avg']]\n",
    "        site['weather_data'] = False\n",
    "    \n",
    "    \n",
    "    df.sort_values('time', inplace=True)\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        row_24h = df[(df['time'] == row[1]['time'] - pd.Timedelta(hours=24)) & (df['value_key'] == row[1]['value_key'])]\n",
    "\n",
    "        if len(row_24h) != 0:\n",
    "            df.loc[row[0], 'avg_24h'] = row_24h['avg'].values[0]\n",
    "        else:\n",
    "            df.loc[row[0], 'avg_24h'] = np.nan\n",
    "\n",
    "    days = df['time'].dt.date.unique()\n",
    "    for day in days:\n",
    "        for value_key in df['value_key'].unique():\n",
    "            day_before = day - pd.Timedelta(days=1)\n",
    "            power_values = df[(df['time'].dt.date == day_before) & (df['value_key'] == value_key)]['avg'].values\n",
    "            num_rows = len(df[(df['time'].dt.date == day) & (df['value_key'] == value_key)])\n",
    "            if any([x is None for x in power_values]) or num_rows != 96:\n",
    "                acc_energy = [None] * num_rows\n",
    "                day_energy = [None] * num_rows\n",
    "            else:\n",
    "                acc_energy = accumulated_day_energy(power_values)\n",
    "                day_energy = whole_day_energy(power_values)\n",
    "                \n",
    "            df.loc[(df['time'].dt.date == day) & (df['value_key'] == value_key), 'accumulated_energy_24h'] = acc_energy    \n",
    "            df.loc[(df['time'].dt.date == day) & (df['value_key'] == value_key), 'whole_day_energy_24h'] = day_energy\n",
    "\n",
    "    os.makedirs(f'../data/test_data/{site_id}', exist_ok=True)\n",
    "    distinct_value_keys = df['value_key'].unique()\n",
    "    for value_key in distinct_value_keys:\n",
    "        filtered_df = df[df['value_key'] == value_key]\n",
    "        filtered_df.to_csv(f'../data/test_data/{site_id}/{value_key}.csv', index=False)\n",
    "\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    return df, site['weather_data']\n",
    "\n",
    "pool = Pool(8)\n",
    "results = pool.map(process_site, site_info['site_id'])\n",
    "df_all = pd.concat([result[0] for result in results], ignore_index=True)\n",
    "site_info['weather_data'] = [result[1] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load production data from disk\n",
    "\n",
    "site_infos = pd.read_csv(f'../data/test_data/site_info.csv')\n",
    "\n",
    "power_data = pd.DataFrame()\n",
    "\n",
    "for index, row in site_infos.iterrows():\n",
    "    site_id = row['site_id']\n",
    "    value_keys = ['POWER_PRODUCTION']\n",
    "    for value_key in value_keys:\n",
    "        if os.path.exists(f'../data/test_data/{site_id}/{value_key}.csv'):\n",
    "            df = pd.read_csv(f'../data/test_data/{site_id}/{value_key}.csv')\n",
    "            power_data = pd.concat([power_data, df], ignore_index=True)\n",
    "\n",
    "\n",
    "power_data['time'] = pd.to_datetime(power_data['time'])\n",
    "df_all = power_data\n",
    "\n",
    "site_info = site_infos\n",
    "\n",
    "print(f'Data length: {len(power_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get orientation clusters\n",
    "# The timeframe from May to July is chosen for data collection due to the Sun's position in Central Europe. During these months, the Sun is at its highest point, \n",
    "# providing maximum sunlight exposure. This leads to more accurate and reliable data, particularly in studies related to solar energy and climate research\n",
    "\n",
    "# get perfect day in summer months\n",
    "def _get_perfect_days(df):\n",
    "    # iterate over all days\n",
    "    days = pd.to_datetime(df['time']).dt.date.unique()\n",
    "    \n",
    "    # get days in May, June, July\n",
    "    days = [day for day in days if day.month in [5, 6, 7]]\n",
    "\n",
    "    perfect_days = []\n",
    "\n",
    "    for day in days:\n",
    "        # select values between 6 am and 9 pm (6-21)\n",
    "        relevant_data_of_day = df[df['time'].dt.date == day]\n",
    "        if not len(relevant_data_of_day) == 96:\n",
    "            # print(f'Not all values available for {day} and site {site_id}, actual length: {len(relevant_data_of_day)}')\n",
    "            continue\n",
    "\n",
    "        relevant_data_of_day = relevant_data_of_day[(relevant_data_of_day['time'].dt.hour >= 6) & (relevant_data_of_day['time'].dt.hour <= 21)]\n",
    "\n",
    "        # get days with practically no clouds (no value over 10%) between 6 am and 9 pm (6-21)\n",
    "        if not (relevant_data_of_day[relevant_data_of_day['clouds'] > 35].empty):\n",
    "            # print(f'Clouds over 10% for {day} and site {site_id}, max clouds: {relevant_data_of_day[\"clouds\"].max()}')\n",
    "            continue\n",
    "\n",
    "        perfect_days.append(day)\n",
    "\n",
    "    print(f'Perfect days for site {site_id}: {len(perfect_days)}')       \n",
    "    return perfect_days\n",
    "\n",
    "def _median_time_of_max_values_of_perfect_days(df, column):\n",
    "    perfect_days = _get_perfect_days(df)\n",
    "    \n",
    "    times_max_value = []\n",
    "    for day in perfect_days:\n",
    "        day_data = df[df_all['time'].dt.date == day]\n",
    "        max_value = day_data[column].max()\n",
    "        if np.isnan(max_value):\n",
    "            continue\n",
    "        time_max_value = day_data[day_data[column] == max_value]['time'].values[0]\n",
    "        # converto to datetime object\n",
    "        time_max_value = pd.to_datetime(time_max_value)\n",
    "        times_max_value.append(time_max_value)\n",
    "\n",
    "    times_max_value = [time.hour * 60 + time.minute for time in times_max_value]\n",
    "    median_time = np.median(times_max_value)\n",
    "\n",
    "    return median_time\n",
    "\n",
    "def _median_value_for_time(df, column, hour, minute = 0):\n",
    "    perfect_days = _get_perfect_days(df)\n",
    "    \n",
    "    values = []\n",
    "    for day in perfect_days:\n",
    "        day_data = df[df_all['time'].dt.date == day]\n",
    "        time = pd.Timestamp(year=day.year, month=day.month, day=day.day, hour=hour, minute=minute)\n",
    "        value = day_data[day_data['time'] == time][column].values\n",
    "        if len(value) == 0:\n",
    "            continue\n",
    "\n",
    "        values.append(value[0])\n",
    "        \n",
    "    return np.median(values)\n",
    "\n",
    "def _median_hour_max_energy_per_hour(df):\n",
    "    perfect_days = _get_perfect_days(df)\n",
    "    \n",
    "    hour_max_value = []\n",
    "    for day in perfect_days:\n",
    "        day_data = df[df_all['time'].dt.date == day]\n",
    "        \n",
    "        max_hour = 0\n",
    "        max_hour_value = 0\n",
    "        for hour in range(1, 23):\n",
    "            hour_energy = day_data[day_data['time'].dt.hour == hour - 1]['avg'].mean()\n",
    "\n",
    "            if hour_energy > max_hour_value:\n",
    "                max_hour = hour\n",
    "                max_hour_value = hour_energy\n",
    "        \n",
    "        hour_max_value.append(max_hour)\n",
    "        \n",
    "    median_hour = np.median(hour_max_value)\n",
    "\n",
    "    return median_hour\n",
    "\n",
    "sites_with_max_data = []\n",
    "for site_id in site_info['site_id']:\n",
    "    site = site_info[site_info['site_id'] == site_id].iloc[0]\n",
    "    df = df_all[(df_all['site_id'] == site_id) & (df_all['value_key'] == 'POWER_PRODUCTION')]\n",
    "\n",
    "    site['median_time_of_max_solar_rad'] = _median_time_of_max_values_of_perfect_days(df, 'solar_rad')\n",
    "    site['median_time_of_max_avg'] = _median_time_of_max_values_of_perfect_days(df, 'avg')\n",
    "    site['median_hour_max_energy'] = _median_hour_max_energy_per_hour(df)\n",
    "\n",
    "    sites_with_max_data.append(site)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order sites by median time of max values\n",
    "sites_with_max_data = sorted(sites_with_max_data, key=lambda x: x['median_time_of_max_avg'])\n",
    "\n",
    "for site in sites_with_max_data:\n",
    "    print(f'Site ID: {site[\"site_id\"]}, Median time of max solar radiation: {site[\"median_time_of_max_solar_rad\"]}, Median time of max avg: {site[\"median_time_of_max_avg\"]}, Median hour of max energy: {site[\"median_hour_max_energy\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(site_info)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dbscan to cluster sites by median time of max values\n",
    "sites_with_max_data = [site for site in sites_with_max_data if not np.isnan(site[\"median_time_of_max_avg\"])]\n",
    "\n",
    "data = np.array([site[\"median_time_of_max_avg\"] for site in sites_with_max_data]).reshape(-1, 1)\n",
    "\n",
    "# divide values by max (1440 minutes in a day)\n",
    "data = data/1440\n",
    "\n",
    "dbscan = DBSCAN(eps=0.01, min_samples=2)\n",
    "\n",
    "clusters = dbscan.fit_predict(data)\n",
    "\n",
    "for i, site in enumerate(sites_with_max_data):\n",
    "    site_id = site[\"site_id\"]\n",
    "    cluster = clusters[i]\n",
    "    row = site_info[site_info[\"site_id\"] == site_id].iloc[0]\n",
    "    row[\"clusters\"][\"orientation\"] = cluster\n",
    "    \n",
    "# set -1 for sites with no orientation data\n",
    "for site in site_info.iterrows():\n",
    "    if \"orientation\" not in site[1][\"clusters\"]:\n",
    "        site[1][\"clusters\"][\"orientation\"] = -1\n",
    "\n",
    "print(site_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set actual kWp values (inferred from the data)\n",
    "\n",
    "for site in site_info.iterrows():\n",
    "    site_id = site[1][\"site_id\"]\n",
    "    df = df_all[(df_all[\"site_id\"] == site_id) & (df_all[\"value_key\"] == \"POWER_PRODUCTION\")]\n",
    "    \n",
    "    max_power = df[\"avg\"].max()\n",
    "\n",
    "    if max_power == max_power:\n",
    "        kwp = max_power / 1000\n",
    "        # round up\n",
    "        kwp = round(kwp + 0.5)\n",
    "\n",
    "        site_info.loc[site[0], \"kwp\"] = kwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save site_info \n",
    "site_info.to_csv(f'../data/test_data/site_info.csv', index=True)\n",
    "\n",
    "for site in site_info.iterrows():\n",
    "    site_id = site[1][\"site_id\"]\n",
    "    site_info = site[1]\n",
    "    site_info.to_csv(f'../data/test_data/{site_id}/site_info.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 7  # The size of each window (in days)\n",
    "\n",
    "def are_subsequent(dates):\n",
    "    \"\"\"Check if dates are subsequent.\"\"\"\n",
    "    return np.all(np.diff(dates) == pd.Timedelta(days=1))\n",
    "\n",
    "for site_id in site_info['site_id']:\n",
    "    site = site_info[site_info['site_id'] == site_id].iloc[0]\n",
    "    site_power_data = power_data[(power_data['site_id'] == site_id)]\n",
    "\n",
    "    unique_days = np.sort(site_power_data['time'].dt.date.unique())\n",
    "\n",
    "    # Split the unique days into windows\n",
    "    windows = [unique_days[i:i+window_size] for i in range(0, len(unique_days), window_size)]\n",
    "\n",
    "    # Filter out non-subsequent windows\n",
    "    complete_windows = [window for window in windows if len(window) == window_size and are_subsequent(window)]\n",
    "\n",
    "    # Shuffle the windows\n",
    "    random.shuffle(complete_windows)\n",
    "\n",
    "    # Select a fixed number of windows for testing\n",
    "    test_windows_size = 10  # Change this to the number of windows you want for testing\n",
    "    test_windows, training_windows = complete_windows[:test_windows_size], complete_windows[test_windows_size:]\n",
    "\n",
    "    # Sort the windows\n",
    "    test_windows.sort(key=lambda x: x[0])\n",
    "    training_windows.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Convert windows into DataFrame\n",
    "    training_windows_df = pd.DataFrame(training_windows)\n",
    "    test_windows_df = pd.DataFrame(test_windows)\n",
    "\n",
    "    # Write windows to CSV files\n",
    "    training_windows_df.to_csv(f'../data/test_data/{site_id}/training_windows.csv', index=False, header=True)\n",
    "    test_windows_df.to_csv(f'../data/test_data/{site_id}/test_windows.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max values for solar_rad and ghi\n",
    "print(\"Max solar_rad:\", df_all['solar_rad'].max())\n",
    "print(\"Max ghi:\", df_all['ghi'].max())\n",
    "print(\"Max temp:\", df_all['temp'].max())\n",
    "print(\"Max precip:\", df_all['precip'].max())\n",
    "print(\"Max snow depth:\", df_all['snow_depth'].max())\n",
    "print(\"Max clouds:\", df_all['clouds'].max())\n",
    "\n",
    "# min values\n",
    "print(\"Min solar_rad:\", df_all['solar_rad'].min())\n",
    "print(\"Min ghi:\", df_all['ghi'].min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
